{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import embed\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(669)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e027e",
   "metadata": {},
   "source": [
    "### Hooks + VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_params(model):\n",
    "    for name, params in model.named_modules():\n",
    "        print(name)\n",
    "        print(params)\n",
    "        print()   \n",
    "\n",
    "# https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6\n",
    "acts = {}\n",
    "handles = []\n",
    "def get_activation(name, detach):\n",
    "    def hook(model, inputs, outputs):\n",
    "        if detach:\n",
    "            acts[name] = outputs.detach()\n",
    "        else:\n",
    "            acts[name] = outputs\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "vgg = models.vgg19(pretrained=True)\n",
    "# Replace maxpool with avgpool as per: https://stackoverflow.com/a/65429290\n",
    "for i,layer in vgg.features.named_children():\n",
    "    if isinstance(layer, nn.MaxPool2d):\n",
    "        vgg.features[int(i)] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
    "vgg = vgg.to('cuda')\n",
    "# Select layers\n",
    "layers = [\n",
    "          (0,'conv1_1','style'),\n",
    "          (5,'conv2_1','style'),\n",
    "          (10,'conv3_1','style'),\n",
    "          (19,'conv4_1','style'),\n",
    "          (21,'conv4_2','content'),\n",
    "          (28,'conv5_1','style')\n",
    "]\n",
    "# Register forward hooks on layers\n",
    "for layer,layer_name,_ in layers:\n",
    "    h = vgg.features[layer].register_forward_hook(get_activation(layer_name, detach=True))\n",
    "    handles.append(h)\n",
    "    # DOESN'T WORK: Also set vgg's learnable parameters to be non-leaf, aka. not learnable:\n",
    "#     if isinstance(vgg.features[layer],nn.Conv2d):\n",
    "#         embed()\n",
    "#         weight = vgg.features[layer].weight.data.detach()\n",
    "#         weight.requires_grad = True\n",
    "#         weight.is_leaf = False\n",
    "#         del vgg.features[layer].weight\n",
    "#         vgg.features[layer].weight.data = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c4431",
   "metadata": {},
   "source": [
    "### Load content and style targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load style and content images\n",
    "a = Image.open(\"./webb1.png\")\n",
    "a = a.convert('RGB')\n",
    "# a = Image.open(\"./sower.png\")\n",
    "p = Image.open(\"./eee3.png\")\n",
    "p = p.convert('RGB')\n",
    "# Resize\n",
    "\n",
    "import PIL.ImageOps\n",
    "w,h = a.size\n",
    "a = a.crop((750,0,w/2,h/2))\n",
    "print(a.size)\n",
    "print(p.size)\n",
    "a = a.resize((1000,1000))\n",
    "p = p.resize((1000,1000))\n",
    "display(a)\n",
    "display(p)\n",
    "# '''\n",
    "# PIL image to numpy\n",
    "a = np.asarray(a)\n",
    "p = np.asarray(p)[:,:,:3]\n",
    "# Convert to Torch tensor, scale...\n",
    "a = torch.tensor(a, dtype=torch.float32, requires_grad=False, device='cuda') / 255\n",
    "p = torch.tensor(p, dtype=torch.float32, requires_grad=False, device='cuda') / 255\n",
    "# ... and normalize as per: # https://pytorch.org/vision/0.12/models.html\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device='cuda')\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device='cuda')\n",
    "a = (a - mean) / std\n",
    "p = (p - mean) / std\n",
    "\n",
    "# Transpose and get activations\n",
    "from einops import rearrange\n",
    "a = rearrange(a, \"W H C -> 1 C H W\")\n",
    "p = rearrange(p, \"W H C -> 1 C H W\")\n",
    "\n",
    "# Get activation values\n",
    "vgg(a)\n",
    "acts_a = acts\n",
    "acts = {}\n",
    "vgg(p)\n",
    "acts_p = acts\n",
    "acts = {}\n",
    "for i,(layer,name,layer_type) in enumerate(layers):\n",
    "    print(f\"layer #{layer}, {layer_type}\")\n",
    "    print(f\"{name} - {acts_a[name].shape}\")\n",
    "    print(f\"{name} - {acts_a[name].requires_grad}\")\n",
    "    assert torch.equal(acts_a[name],acts_p[name])==False\n",
    "    print()\n",
    "\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee102258",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(acts_x, acts_a, layers):\n",
    "    losses = []\n",
    "    for i,(layer,name,layer_type) in enumerate(layers):\n",
    "        if layer_type=='style':\n",
    "            x_features = torch.squeeze(acts_x[name], dim=0)\n",
    "            a_features = torch.squeeze(acts_a[name], dim=0)\n",
    "            # Flatten spatial dimensions\n",
    "            x_features = rearrange(x_features, \"C H W -> C (H W)\")\n",
    "            a_features = rearrange(a_features, \"C H W -> C (H W)\")\n",
    "            # Gram matrix, einsum(\"C H W, D H W -> C D\", x, x)?\n",
    "            gram_x = torch.einsum(\"C S, S D -> C D\", x_features, x_features.T) # S is for spatial over W*H\n",
    "            gram_a = torch.einsum(\"C S, S D -> C D\", a_features, a_features.T)\n",
    "            loss = F.mse_loss(gram_x, gram_a)\n",
    "            losses.append(loss)\n",
    "    loss_final = sum(losses) / len(losses)\n",
    "    return loss_final\n",
    "            \n",
    "            \n",
    "def content_loss(acts_x, acts_p, layers):\n",
    "    losses = []\n",
    "    for i,(layer,name,layer_type) in enumerate(layers):\n",
    "        if layer_type=='content':\n",
    "            x_features = torch.squeeze(acts_x[name], dim=0)\n",
    "            p_features = torch.squeeze(acts_p[name], dim=0)\n",
    "            loss = F.mse_loss(x_features, p_features)\n",
    "            losses.append(loss)\n",
    "    loss_final = sum(losses) / len(losses)\n",
    "    return loss_final\n",
    "\n",
    "def var_loss(x):\n",
    "    return torch.mean(torch.abs(x))\n",
    "#     width_diff = x[:,:,:,1:] - x[:,:,:,:-1] # ie. col_2 - col_1\n",
    "#     height_diff = x[:,:,1:,:] - x[:,:,:-1,:] # ie. row_2 - row_1\n",
    "#     diag_diff1 = x[:,:,1:,1:] - x[:,:,:-1,:-1]\n",
    "#     diag_diff2 = x[:,:,:-1,1:] - x[:,:,1:,:-1]\n",
    "#     diff_types = [width_diff, height_diff, diag_diff1, diag_diff2]\n",
    "#     loss = 0.0\n",
    "#     for t in diff_types:\n",
    "#         loss = loss + torch.mean(torch.square(t))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec112e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(x, std, mean):\n",
    "    std = std.to('cpu')\n",
    "    mean = mean.to('cpu')\n",
    "    x = rearrange(x, \"B C H W -> W H (C B)\")\n",
    "    x = x * std[None,None,:] + mean[None,None,:]\n",
    "    x = np.clip(x, 0.0, 1.0)\n",
    "    x = x.detach().numpy()*255\n",
    "    x = x.astype(np.uint8)\n",
    "#     x = Image.fromarray(x,'RGB')\n",
    "    return x\n",
    "\n",
    "def pixel_delta(x_old, x_new):\n",
    "    x_old = np.asarray(x_old)\n",
    "    x_new = np.asarray(x_new)\n",
    "    x_delta = np.abs(x_new - x_old)\n",
    "    delta_mean, delta_std = np.mean(x_delta), np.std(x_delta)\n",
    "    print(f\"{delta_mean=},\\n{delta_std=}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129beb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_x = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x, normalize, make into trainable tensor.\n",
    "x = p.clone().detach()\n",
    "x.requires_grad = True  \n",
    "x = x.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9653e14",
   "metadata": {},
   "source": [
    "### Set loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=[x], lr=6e-3, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-8 # Content\n",
    "beta = 1e0   # Style\n",
    "gamma = 1e3  # Pixel variation\n",
    "# Note: set high content weight to initialize x, then switch to emphasize style.\n",
    "# This seems to make colors more coherent (ie. blue on sweater, yellow on skin, etc.)\n",
    "# Finish up with higher gamma to fix artifacts.\n",
    "print(f\"texture var_loss baseline = {var_loss(a)*gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a65ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "render_old = None\n",
    "render_new = unnormalize(x.detach().to('cpu'), std, mean)\n",
    "iter_k = 100\n",
    "for i in tqdm(range(5000+1)):\n",
    "    if i%iter_k==0 and i>0:\n",
    "        render_old = render_new # ie. from (i-100)\n",
    "        render_new = unnormalize(x.detach().to('cpu'), std, mean) # ie. current value i\n",
    "    # Register forward hooks on layers\n",
    "    handles = []\n",
    "    for layer,layer_name,_ in layers:\n",
    "        h = vgg.features[layer].register_forward_hook(get_activation(layer_name, detach=False))\n",
    "        handles.append(h)\n",
    "        \n",
    "    # Run x through vgg, extract activations, get style and content losses\n",
    "    vgg(x)\n",
    "    content_loss_ = alpha*content_loss(acts, acts_p, layers)\n",
    "    style_loss_ = beta*style_loss(acts, acts_a, layers)\n",
    "    var_loss_ = gamma*var_loss(x)\n",
    "    loss = content_loss_ + style_loss_ + var_loss_\n",
    "    # Run backprop, etc.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%iter_k==0 and i>0:\n",
    "        # Losses\n",
    "        print(f\"style loss = {style_loss_}\")\n",
    "        print(f\"content loss = {content_loss_}\")\n",
    "        print(f\"var loss = {var_loss_}\")\n",
    "        # Render before, after, and delta\n",
    "        render_new = unnormalize(x.detach().to('cpu'), std, mean)\n",
    "        pixel_delta(render_old, render_new)\n",
    "        fused = np.concatenate((render_new, render_new-render_old),axis=1)\n",
    "        trained_x.append(Image.fromarray(fused,'RGB')) # Store progress\n",
    "        # Display renders\n",
    "#         render_delta_pil = Image.fromarray(render_new - render_old,'RGB')\n",
    "        render_new_pil = Image.fromarray(render_new,'RGB')\n",
    "#         render_old_pil = Image.fromarray(render_old,'RGB')\n",
    "#         display(render_delta_pil.resize(tuple(int(dim/2) for dim in render_delta_pil.size))) # Delta\n",
    "        display(render_new_pil.resize(tuple(int(dim/1) for dim in render_new_pil.size)))     # New \n",
    "\n",
    "    # Remove handles\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for render in trained_x:\n",
    "    display(render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094a33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
